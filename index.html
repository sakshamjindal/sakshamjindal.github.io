<!DOCTYPE HTML>

<style>
  .regular-text {
    font-size: 16px;
    font-family:Verdana,Geneva,sans-serif;
  }
</style>

<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Saksham Jindal</title>
  
  <meta name="author" content="Saksham Jindal">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Saksham Jindal</name>
              </p>
              <p align="justify"; font-size: 50px>
              I am a Masters student studying ECE at the <a href="https://ucsd.edu" target="_blank"><font size="3">University of California San Diego</font></a>. 
              </p>
              <p align="justify">
              </p>
              <p style="text-align:center">
                <a href="mailto:sjindal@ucsd.edu">Email</a> &nbsp/&nbsp
                <a href="data/SakshamJindal-Resume.pdf">Resume</a> &nbsp/&nbsp
                <a href="https://linkedin.com/in/sakshamjinda">Linkedin</a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <!-- <a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Google Scholar</a> &nbsp/&nbsp -->
                <a href="https://twitter.com/SAKSHAM111">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/sakshamjindal/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/SakshamJindal.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/SakshamJindal.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <!-- Experience - Heading -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Professional Experience</heading>
                <!-- <p>
                  I'm interested in computer vision, machine learning, optimization, and image processing. Much of my research is about inferring the physical world (shape, motion, color, light, etc) from images. Representative papers are <span class="highlight">highlighted</span>.
                </p> -->
              </td>
            </tr>
          </tbody>
        </table>
      
        <!-- Experience - Content -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <table border=0 class="bg_colour" style="padding:20px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>  
                <tr>
                    <td style="padding:10px;width:25%;vertical-align:middle;padding-top: 20px;">
                        <div class="one">
                            <img src='images/tomtom.png' width="120">
                        </div>
                    </td>
                    <td style="padding:10px;width:75%;vertical-align:top">
                        <papertitle style="color:rgb(110, 110, 110)"><big>Senior Data Scientist</big> </papertitle> <papertitle ><big> | TomTom, Inc.</big></papertitle>
                        <br>
                        November 2021 - August 2022
                        <br>
                        <p align="justify">Worked with the Palette AI Research (PAIR) team at 
                          <a href="https://advertising.amazon.com" target="_blank">Amazon Advertising</a> on building machine learning models that can automatically
                          create high-quality and high-performance video advertisements.</p>
                          <a href="https://advertising.amazon.com" target="_blank">Amazon Advertising</a> on building machine learning models that can automatically
                          create high-quality and high-performance video advertisements.</p>
                          <a href="https://advertising.amazon.com" target="_blank">Amazon Advertising</a> on building machine learning models that can automatically
                          create high-quality and high-performance video advertisements.</p>

                    </td>
                </tr>

                <tr>
                    <td style="padding:10px;width:25%;vertical-align:middle;padding-top: 20px;">
                        <div class="one">
                            <img src='images/fractal.png' width="120">
                        </div>
                    </td>
                    <td style="padding:10px;width:75%;vertical-align:top">
                        <papertitle style="color:rgb(110, 110, 110)"><big>Data Scientist II </big> </papertitle> <papertitle ><big> | Fractal, Inc.</big></papertitle>
                        <br>
                        June 2018 - October 2021
                        <br>
                        <p align="justify">Worked with the Palette AI Research (PAIR) team at 
                          <a href="https://advertising.amazon.com" target="_blank">Amazon Advertising</a> on building machine learning models that can automatically
                          create high-quality and high-performance video advertisements.</p>
                    </td>
                </tr>
              </tbody>
            </table>
          </div>
        <!-- <hr class="soft"> -->

        <!-- Research - Heading -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Research Experience</heading>
                <!-- <p>
                  I'm interested in computer vision, machine learning, optimization, and image processing. Much of my research is about inferring the physical world (shape, motion, color, light, etc) from images. Representative papers are <span class="highlight">highlighted</span>.
                </p> -->
              </td>
            </tr>
          </tbody>
        </table>
      
        <!-- Research - Content -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <table border=0 class="bg_colour" style="padding:20px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>  
                <tr>
                    <td style="padding:10px;width:25%;vertical-align:middle;padding-top: 20px;">
                        <div class="one">
                            <img src='images/ucsd2.png' width="120">
                        </div>
                    </td>
                    <td style="padding:10px;width:75%;vertical-align:top">
                        <papertitle style="color:rgb(110, 110, 110)"><big>Graduate Research Asssistant</big> </papertitle> <papertitle ><big> | TomTom, Inc.</big></papertitle>
                        <br>
                        January 2023 - Present
                        <br>
                        <p align="justify">Worked with the Palette AI Research (PAIR) team at 
                          <a href="https://advertising.amazon.com" target="_blank">Amazon Advertising</a> on building machine learning models that can automatically
                          create high-quality and high-performance video advertisements.</p>
                    </td>
                </tr>

                <tr>
                  <td style="padding:10px;width:25%;vertical-align:middle;padding-top: 20px;">
                      <div class="one">
                          <img src='images/iiith.png' width="120">
                      </div>
                  </td>
                  <td style="padding:10px;width:75%;vertical-align:top">
                      <papertitle style="color:rgb(110, 110, 110)"><big>Research Intern</big> </papertitle> <papertitle ><big> | Robotics Research Centre, IIIT Hyderabad </big></papertitle>
                      <br>
                      April 2021 - December 2021
                      <br>
                      <p align="justify">Worked with the Palette AI Research (PAIR) team at 
                        <a href="https://advertising.amazon.com" target="_blank">Amazon Advertising</a> on building machine learning models that can automatically
                        create high-quality and high-performance video advertisements.</p>
                  </td>
                </tr>
              </tbody>
            </table>
          </div>
        <!-- <hr class="soft"> -->
          
      <!-- Open Source - Heading -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Open Source Projects</heading>
                <!-- <p>
                  I'm interested in computer vision, machine learning, optimization, and image processing. Much of my research is about inferring the physical world (shape, motion, color, light, etc) from images. Representative papers are <span class="highlight">highlighted</span>.
                </p> -->
              </td>
            </tr>
          </tbody>
        </table>

       <!-- Open Source - Contents -->
        <table border=0 class="bg_colour" style="padding:20px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>

        <!-- Diffusion Project -->
            <tr>
                <td style="padding:10px;width:25%;vertical-align:middle;padding-top: 20px;">
                    <div class="one">
                        <img src='images/diffusion_arch.png' width="120">
                    </div>
                </td>
                <td style="padding:10px;width:75%;vertical-align:top">
                    <papertitle style="color:black">Controllable subject guided
                      text-to-image generation and editing using diffusion models 
                    </papertitle>
                    <br>
                    <a href = 'data/Diffusion_Dreambooth.pdf'> Paper</a> /
                    <a href = 'https://openaccess.thecvf.com/content/WACV2022/supplemental/Karmali_LEAD_Self-Supervised_Landmark_WACV_2022_supplemental.pdf'> Code </a>             
                    <br>
                    <br>
                    Developed pipeline for personalized text-to-image generation and editing using latent diffusion models 
                    (generative AI), incorporating LoRA finetuning for image generation and cross-attention guidance for image editing
                    <i>(pytorch / hugging face diffusers /stable diffusion) </i>
                    <br> 
                    <br>
                    <!-- # make italic -->
                    <i>Keywords: </i> pytorch / hugging face diffusers / stable diffusion / latent diffusion / cross attention
                    
                </td> 
            </tr>
              <!-- Visual Odometry Project -->
              <tr>
                <td style="padding:10px;width:25%;vertical-align:middle;padding-top: 20px;">
                    <div class="one">
                        <img src='images/vo-slam.gif' width="120">
                    </div>
                </td>
                <td style="padding:10px;width:75%;vertical-align:top">
                    <papertitle style="color:black">Stereo visual SLAM with local bundle adjustment on KITTI dataset </papertitle>
                    <br>
                    <a href = 'https://openaccess.thecvf.com/content/WACV2022/supplemental/Karmali_LEAD_Self-Supervised_Landmark_WACV_2022_supplemental.pdf'> Code </a>             
                    <br>
                    <br>
                    Implemented a Visual SLAM pipeline for 6-DOF camera pose estimation and outdoor scene mapping using techniques 
                    in multi-view geometry - RANSAC, feature tracking, 3D reconstruction, pose estimation (PnP algorithm) and 
                    bundle adjustment
                    <br> 
                    <br>
                </td>
            </tr>

          <!-- Satellite Image Segmentation -->
            <tr>
                <td style="padding:10px;width:25%;vertical-align:middle;padding-top: 20px;">
                    <div class="one">
                        <img src='images/satellite.png' width="120">
                    </div>
                </td>
                <td style="padding:10px;width:75%;vertical-align:top">
                    <papertitle style="color:black"> How to Extract Roads from Satellite Images using Semantic Segmentation? </papertitle>
                    <br>
                    <a href = 'data/Road_Extraction_from_Satellite_Images.pdf'> Paper</a> /
                    <a href = 'https://openaccess.thecvf.com/content/WACV2022/supplemental/Karmali_LEAD_Self-Supervised_Landmark_WACV_2022_supplemental.pdf'> Code </a>             
                    <br>
                    <br>
                    Developed framework for extraction of road geometry from satellite images using convolutional neural network (CNN) 
                    based semantic segmentation architectures experimenting with data augmentations, schedulers, optimizers and loss functions 
                    <br> 
                    <br>
                </td>
            </tr>

          <!-- Particle Filter SLAM -->
            <tr>
                <td style="padding:10px;width:25%;vertical-align:middle;padding-top: 20px;">
                    <div class="one">
                        <img src='images/pf-slam.png' width="120">
                    </div>
                </td>
                <td style="padding:10px;width:75%;vertical-align:top">
                    <papertitle style="color:black">Particle Filter SLAM for an indoor differential-drive robot </papertitle>
                    <br>
                    <a href = 'data/PF-SLAM.pdf'> Paper</a> /
                    <a href = 'https://openaccess.thecvf.com/content/WACV2022/supplemental/Karmali_LEAD_Self-Supervised_Landmark_WACV_2022_supplemental.pdf'> Code </a>             
                    <br>
                    <br>
                    Created Particle Filter SLAM pipeline using IMU odometry data and LiDAR scans from sensors mounted on the 
                    differential drive robot to enable localization and build occupancy grid map of the environment 
                    <br> 
                    <br>
                </td>
            </tr>

          <!-- Particle Filter SLAM -->
            <!-- <tr>
              <td style="padding:10px;width:25%;vertical-align:middle;padding-top: 20px;">
                  <div class="one">
                      <img src='images/diffusion_arch.png' width="120">
                  </div>
              </td>
              <td style="padding:10px;width:75%;vertical-align:top">
                  <papertitle style="color:black">LEAD: Self-Supervised Landmark Estimation 
                    by Aligning Distributions of Feature Similarity </papertitle>
                  <br>
                  <a href = 'https://openaccess.thecvf.com/content/WACV2022/papers/Karmali_LEAD_Self-Supervised_Landmark_Estimation_by_Aligning_Distributions_of_Feature_Similarity_WACV_2022_paper.pdf'> Paper</a> /
                  <a href = 'https://openaccess.thecvf.com/content/WACV2022/supplemental/Karmali_LEAD_Self-Supervised_Landmark_WACV_2022_supplemental.pdf'> Code </a>             
                  <br>
                  <p align="justify">Tejan Karmali*, Abhinav Atrishi*, <b>Sai Sree Harsha</b>, Susmit Agrawal, Varun Jampani, Venkatesh Babu</p>
                  <br>
              </td>
            </tr> -->

            <!-- Variational Autoencoder -->
            <tr>
              <td style="padding:10px;width:25%;vertical-align:middle;padding-top: 20px;">
                  <div class="one">
                      <img src='images/medvae.png' width="120">
                  </div>
              </td>
              <td style="padding:10px;width:75%;vertical-align:top">
                  <papertitle style="color:black"> MedVAE: Generating Chest X-Ray Images using
                    Variational Autoencoders (VAE) </papertitle>
                  <br>
                  <a href = 'data/MedVAE.pdf'> Paper</a> /
                  <a href = 'https://openaccess.thecvf.com/content/WACV2022/supplemental/Karmali_LEAD_Self-Supervised_Landmark_WACV_2022_supplemental.pdf'> Code </a>             
                  <br>
                  <br>
                  Programmed variational auto-encoder (VAE) trained on a dataset of chest X-rays from patients diagnosed with pneumonia
                  to generate realistic and representative chest X-ray images for improved diagnostic
                  <br> 
                  <br>
              </td>
            </tr>

            <!-- Generative Adversial Network -->
            <tr>
              <td style="padding:10px;width:25%;vertical-align:middle;padding-top: 20px;">
                  <div class="one">
                      <img src='images/art-gan.png' width="120">
                  </div>
              </td>
              <td style="padding:10px;width:75%;vertical-align:top">
                  <papertitle style="color:black"> GAN-Art: Generating Artwork using Deep
                    Convolutional Generative Adversarial Networks (DCGANs) </papertitle>
                  <br>
                  <a href = 'data/GAN-Art.pdf'> Paper</a> /
                  <a href = 'https://openaccess.thecvf.com/content/WACV2022/supplemental/Karmali_LEAD_Self-Supervised_Landmark_WACV_2022_supplemental.pdf'> Code </a>             
                  <br>
                  <br>
                  Programmed deep convolutional generative adversarial network (DCGAN) trained on 50,000+ images from 10 different artistic styles
                  to generate high-quality and realistic artwork images that closely resemble the characteristics of each artistic style
                  <br> 
                  <br>
              </td>
            </tr>

            <!-- Orientation Tracking -->
            <tr>
              <td style="padding:10px;width:25%;vertical-align:middle;padding-top: 20px;">
                  <div class="one">
                      <img src='images/orientation-slam.png' width="180">
                  </div>
              </td>
              <td style="padding:10px;width:75%;vertical-align:top">
                  <papertitle style="color:black">
                    Optimised orientation tracking using Riemann stochastic gradient descent
                  </papertitle>
                  <br>
                  <a href = 'data/optimised_orientation_tracking.pdf'> Paper</a> /
                  <a href = 'https://openaccess.thecvf.com/content/WACV2022/supplemental/Karmali_LEAD_Self-Supervised_Landmark_WACV_2022_supplemental.pdf'> Code </a>             
                  <br>
                  <br>
                  Worked on orientation tracking of a rotating body using quartenion kinematics on IMU data, constrained optimization 
                  using Riemannian stochastic gradient descent and generating 360-degree spherical panorama of the indoor scene 
                  <br> 
                  <br>
              </td>
            </tr>

              <!-- Contrastive Learning -->
              <tr>
                <td style="padding:10px;width:25%;vertical-align:middle;padding-top: 20px;">
                    <div class="one">
                        <img src='images/contrastive2.png' width="120">
                    </div>
                </td>
                <td style="padding:10px;width:75%;vertical-align:top">
                    <papertitle style="color:black"> 
                    Object-centric visual and spatial representations on multi-view CLEVR dataset
                    </papertitle>
                    <br>
                    <a href = 'data/Diffusion_Dreambooth.pdf'> Doc</a> /
                    <a href = 'https://openaccess.thecvf.com/content/WACV2022/supplemental/Karmali_LEAD_Self-Supervised_Landmark_WACV_2022_supplemental.pdf'> Code </a>             
                    <br>
                    <br>
                    Researched on momentum contrastive learning to build view invariant visual and view-dependent spatial object centric embeddings to build scene graph on multi-view CLEVR dataset 
                    <br> 
                    <br>
                </td>
              </tr>



          </tbody>
        </table>

        <!-- Footer - Template Credits -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:0px">
                    <br>
                    <p style="text-align:center;font-size:small;"> 
                      <a href="https://jonbarron.info/" target="_blank">Template credits</a>
                    </p>
                  </td>
                </tr>
              </tbody></table>
      
            </td>
          </tr>
        </table>



</body>

</html>
